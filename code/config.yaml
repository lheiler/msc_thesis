method: c22
# Dataset corpus to use: 'tuh' or 'harvard'
data_corp: tuh

paths:
  # Path to TUH EEG training data (ignored for Harvard)
  data_train: "/Users/lorenzheiler/Desktop/thesis/1_Code/thesis/Datasets/tuh-eeg-ab-clean/train"
  # Path to TUH EEG evaluation data (ignored for Harvard)
  data_eval: "/Users/lorenzheiler/Desktop/thesis/1_Code/thesis/Datasets/tuh-eeg-ab-clean/eval"
  # Path to cleaned Harvard EEG BIDS root directory (ignored for TUH)
  data_harvard: "/rds/general/user/lrh24/ephemeral/harvard-eeg/EEG/bids_500_normal_abnormal_clean"
  # Directory where all results will be written
  results_root: "Results"

model:
  # Only batch size is needed when Optuna search is enabled
  batch_size: 16

# Hyper-parameter optimisation via Optuna (overrides manual settings)
optuna:
  n_trials: 50     # Number of optimisation trials per task
  val_split: 0.2   # Fraction of training data reserved for validation
  patience: 20     # Early-stopping patience within each trial

# Whether to *re-extract* latent representations even when cached files exist
reset: false

