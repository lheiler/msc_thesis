method: cwat
# Dataset corpus to use: 'tuh' or 'harvard'
data_corp: tuh

paths:
  # Path to TUH EEG training data (ignored for Harvard)
  data_tuh: "/homes/lrh24/thesis/Datasets/tuh-eeg-ab-clean"
  # Path to cleaned Harvard EEG BIDS root directory (ignored for TUH)
  data_harvard: "/rds/general/user/lrh24/ephemeral/harvard-eeg/EEG/bids_500_normal_abnormal_clean"
  # Directory where all results will be written
  results_root: "Results"

model:
  # "mlp"  (default)  or  "transformer"
  classifier: mlp
  # Only consulted when classifier == transformer
  transformer:
    n_channels: 19      # latent = n_channels * d_model  (auto-infers d_model if omitted)
    d_model:    64     # optional – if omitted it is inferred from input_dim/n_channels
    n_head:       4
    n_layer:      2

  # Only batch size is needed when Optuna search is enabled
  batch_size: 16
  # Hidden layer sizes for the task-specific MLP classifier (only used when
  # Optuna is *disabled*). Provide a single int or a list – they will be used
  # in order as [in → h1 → h2 → … → last].
  hidden_dims: [64]

  # Dropout probability applied after each hidden layer.
  dropout: 0.1

  # Number of training epochs per task (ignored during Optuna search).
  num_epochs: 30

# Hyper-parameter optimisation via Optuna (overrides manual settings)
# optuna:
#   n_trials: 1    # Number of optimisation trials per task
#   val_split: 0.2   # Fraction of training data reserved for validation
#   patience: 20     # Early-stopping patience within each trial

# Whether to *re-extract* latent representations even when cached files exist
reset: false

