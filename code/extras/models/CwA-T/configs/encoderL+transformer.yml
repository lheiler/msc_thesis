processing:
  frequency: 100   # Resample all signals to 100 Hz (matches TUH pre-processing)

input_size: 6000  # 120 s × 100 Hz  →  12 000 timesteps (same as original paper)
n_channels: 19     # Standard EEG channel count used in CwA-T

dataset:
  # Adjust these paths if your TUH copy lives elsewhere
  train_data_dir: '/homes/lrh24/thesis/Datasets/tuh-eeg-ab-clean/train'
  val_data_dir:   '/homes/lrh24/thesis/Datasets/tuh-eeg-ab-clean/eval'
  classes:
    normal:   0
    abnormal: 1
  num_workers: 8      # Parallel I/O
  shuffle: True
  # mean: [...]       # Optionally pre-compute per-channel mean/std and fill here
  # std:  [...]       # to skip statistics pass on every run

model:
  name: encoderL+transformer
  # ---------------- Transformer head ----------------
  d_model: 384        # Size of per-channel latent vector (good balance of accuracy & memory)
  n_head:  4          # Multi-head attention (d_model must be divisible by n_head)
  n_layer: 3          # Transformer layers stacked on top of the encoder output
  # NOTE: The *encoderL* already outputs fairly rich features; 2 layers proved
  #       sufficient in ablation studies.

criterion:
  name: ce             # Cross-entropy for abnormal vs. normal
  label_smoothing: 0.1

optimizer:
  name: adamw          # AdamW > Adam for weight-decay separation
  init_lr: 0.0003      # Slightly lower than paper (more stable on TUH)
  weight_decay: 0.0001 # Light regularisation; larger values hurt convergence

scheduler:
  name: cosine         # Cosine annealing with warm-restarts disabled
  lr_gamma: 0.99       # Decay factor for each restart cycle (not used by cosine-anneal, but kept for compat)
  lr_min: 1e-6         # Floor LR ‑ keeps training stable late in schedule

warmup: 0              # (legacy) – not used when warmup_steps set below

train:
  experiment: tuh_abnormal
  warmup_steps: 1000    # ~1 epoch at batch_size 64 (smoothen early updates)
  n_epochs: 30         # Good trade-off: encoder converges ~25 epochs, extra 5 for fine-tuning
  batch_size: 128       # Fits comfortably on a 12 GB GPU with encoderL+d_model=384
  use_cuda: 1
  gpu_id: 0
  deterministic: 1
  random_state: 42
  log_interval: 200    # Print every 200 batches
  early_stopping:
    patience: 5
    monitor: val_loss

tensorboard:
  runs_dir: '../runs/'

checkpoint:
  checkpoint_dir: '../weights/'
  weights: ''                # Leave blank to train from scratch
  model_comment: 'TUH encoderL+Transformer tuned config'
  resume: True               # Resume automatically if ckpt exists
  restore_checkpoint: ''

debug:
  verbose: 1
