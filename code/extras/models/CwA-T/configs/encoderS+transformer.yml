processing:
  frequency: 128



input_size: 7680
n_channels: 19

dataset:
  name: tuh_abnormal
  train_edf_dir: '/homes/lrh24/thesis/Datasets/tuh-eeg-ab-clean/train/'
  train_data_dir: '/homes/lrh24/thesis/Datasets/tuh-eeg-ab-clean/train/'
  
  val_edf_dir: '/homes/lrh24/thesis/Datasets/tuh-eeg-ab-clean/eval/'
  val_data_dir: '/homes/lrh24/thesis/Datasets/tuh-eeg-ab-clean/eval/'
  
  classes:
    normal: 0
    abnormal: 1
    
  num_workers: 16
  shuffle: True
  preload: True
  
model:
  name: encoderM+transformer    # keep same encoder backbone
  d_model: 160                  # slightly smaller latent size
  n_head: 2                     # divisible by d_model
  n_layer: 0                    # (ignored)
  dropout: 0.35
  classifier: mlp_1l            # <- switch head to 1-layer MLP
  

criterion:
  name: ce
  label_smoothing: 0.05

optimizer:
  name: adam
  init_lr: 2e-5                 # slower than 1e-4 but faster than 3e-5
  weight_decay: 0.03
  betas: [0.9, 0.999]
  eps: 1e-8

scheduler:
  name: cosine
  lr_min: 5e-7

warmup: 1

train:
  experiment: tuh_abnormal
  warmup_steps: 300
  n_epochs: 50
  batch_size: 32
  use_cuda: 1
  gpu_id: 0
  deterministic: 1
  random_state: 42
  log_interval: 200
  early_stopping:
    patience: 15
    monitor: val_acc

tensorboard:
  runs_dir: '../runs/'

checkpoint:
  checkpoint_dir: "../weights/"
  weights: ""  # leave empty; training script won't load weights
  model_comment: ""
  resume: False
  restore_checkpoint: ""

debug:
  verbose: 1

debug_overfit: 0